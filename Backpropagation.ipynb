{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32892254",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we'll delve into the concept of backpropagation in neural networks. Backpropagation is a key algorithm for training neural networks, enabling them to learn from data by adjusting their weights and biases. We'll explore the implementation of backpropagation in the provided neural network code, understand how it computes gradients, and updates the network parameters to minimize the loss function.\n",
    "\n",
    "Before we proceed, ensure that you have the NumPy library installed. If not, you can install it using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3e38628",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: numpy in c:\\python311\\lib\\site-packages (1.26.2)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86f7a35",
   "metadata": {},
   "source": [
    "Now to recap, here's our full class of Network from our previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42bdc755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data):\n",
    "\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "\n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test))\n",
    "            \n",
    "            else:\n",
    "                print(\"Epoch {} complete\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40256353",
   "metadata": {},
   "source": [
    "Now, let's break down the backpropagation part of this class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81d022f",
   "metadata": {},
   "source": [
    "# 1. Initialization\n",
    "First, let's start by initializing the neural network with random weights and biases. We'll create an instance of the Network class with a specified architecture (number of layers and neurons in each layer) and random initialization for weights and biases.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cc9fc9",
   "metadata": {},
   "source": [
    "# 2. Backpropagation Algorithm\n",
    "Now, we'll explore the backpropagation algorithm, which computes the gradients of the cost function with respect to the weights and biases of the network. We'll understand how errors are propagated backward through the network to update the parameters.\n",
    "\n",
    "So the first thing we need to understand is that the main purpose of the backpropagation is to find the rate of change of the error function, with respect to the weights and biases. For this example, we'll talk about the weights only, but do keep n mind that the biases also behave the same way, only a little bit simpler.\n",
    "\n",
    "We''l take the example of neural network with 3 neurons in the input layer, 2 neurons in the hidden layer and one neuron in the output layer. Let's say we want to change the weight between the first neuron of the hidden layer and the neuron of the output layer. Here's the mathemathical explaination of it (note that the superscript denote the layer, while the subscript denote the position of neuron from the top):\n",
    "\n",
    "![Backpropagation algorithm fro output neuron](./images/2.1.jpg)\n",
    "\n",
    "Now, thats-'s just for the output layer. To change the weights of other layer(s), the process is similar but in this case the total error is the sum of error of the neurons in that layer. Hence, the derivative of the error function would be broken down like this:\n",
    "\n",
    " ![Backpropagation algorithm for other neuron](./images/2.2.jpg)\n",
    "\n",
    " If you're still confused, I found [this](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/) to be really helpful!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07abb973",
   "metadata": {},
   "source": [
    "Here's it's implementation in code:\n",
    "\n",
    "```python\n",
    "def backprop(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # forward propagation\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward propagation\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "```\n",
    "\n",
    "This method takes the output of the forward propagation and the desired output, labeled x and y respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd794f5",
   "metadata": {},
   "source": [
    "First, it intializes nabla_b and nabla_w, which is just numpy array of changes we need to make to the weights and biases.Of course, initially all the elements in it will be zero\n",
    "\n",
    "```python\n",
    "nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dee20c",
   "metadata": {},
   "source": [
    "Next, forward propagation start to find an array that represent the output that the network give prior to the adjustmentof the weights and biases. This should be self-explanatory by now. If not, i suggest you to reread the FOrward Propagation notebook.\n",
    "\n",
    "```python\n",
    "# forward propagation\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db04c1b",
   "metadata": {},
   "source": [
    "Now, the backpropagation algorithm starts.\n",
    "\n",
    "```python\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "```\n",
    "First, we start by finding the derivative of the cost function using the method cost_derivative, which given as follows:\n",
    "```python\n",
    "def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "```\n",
    "\n",
    "This basically mean the delta is equal to the cost derivative times with the derivative of the sigmoid function:\n",
    "```python\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "```\n",
    "\n",
    "Note that these functions(cost_derivative and sigmoid_prime is derived from their formula) Since we use the mean squared error (MSE) cost function, the derivative of the cost function is just the difference between the output we get witht he output we want. The same goes with the sigmoid function and it's derivative(sigmoid_prime)\n",
    "\n",
    "Now,we can directly set the last element of the nabla_b with delta, but for nabla_w we must find the dot product between delta and the activations of the hidden layer before the output layer, which is where the activation[-2], which is the derivative of total error with respect to input of the output layer.However, we need to transpose the activations[-2] first to make sure its shape is aligned with delta or otherwise the dot product could not be calculated\n",
    "\n",
    "```python\n",
    "nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "```\n",
    "\n",
    "Now, since we already calculated the nabla_b and nabla_w of the last layer, let's proceed with the rest of the layers:\n",
    "\n",
    "```python\n",
    "for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "```\n",
    "\n",
    "And finally, the backpropagation method returns the the nabla_b and nabla_w whenver it's called."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc642bca",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Update Rule\n",
    "Finally, we'll implement the update rule to adjust the weights and biases of the network using the computed gradients and a learning rate.This is done in the update_mini_batch method:\n",
    "\n",
    "```python\n",
    "def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "```\n",
    "\n",
    "As you can see, the returned value of nabla_b and nabla_w is used to update the weights and biases by substracting the weights and biases with the new weights and biases times the learning rate, eta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0472d8e6",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Backpropagation is a fundamental algorithm for training neural networks, enabling them to learn from data and make accurate predictions. In this notebook, we explored the implementation of backpropagation in the provided neural network code, understanding its role in optimizing network parameters and improving performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
