{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32892254",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we'll delve into the concept of backpropagation in neural networks. Backpropagation is a key algorithm for training neural networks, enabling them to learn from data by adjusting their weights and biases. We'll explore the implementation of backpropagation in the provided neural network code, understand how it computes gradients, and updates the network parameters to minimize the loss function.\n",
    "\n",
    "Before we proceed, ensure that you have the NumPy library installed. If not, you can install it using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3e38628",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\python311\\lib\\site-packages (1.26.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86f7a35",
   "metadata": {},
   "source": [
    "Now to recap, here's our full class of Network from our previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42bdc755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data):\n",
    "\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "\n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                #print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test))\n",
    "                if (j== epochs-1):\n",
    "                    accuracy = self.evaluate(test_data)/n_test\n",
    "                    print(\"Accuracy: {}\".format(accuracy))\n",
    "                    return accuracy\n",
    "            \n",
    "            else:\n",
    "                print(\"Epoch {} complete\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40256353",
   "metadata": {},
   "source": [
    "Now, let's break down the backpropagation part of this class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81d022f",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "# 1. Initialization\n",
    "We'll start by initializing the neural network with random weights and biases. We'll create an instance of the Network class with a specified architecture and random initialization for weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f3da88",
   "metadata": {},
   "source": [
    "# 2. Feedforward Function\n",
    "Next, we'll review the feedforward method, which performs forward propagation through the network to compute the output. This method takes an input vector and passes it forward through each layer, applying the appropriate weights and biases.\n",
    "\n",
    "# Code snippet for the feedforward method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fc631a",
   "metadata": {},
   "source": [
    "# 3. Cost Function\n",
    "Before diving into backpropagation, we'll define the cost function used to measure the network's performance. We'll use the mean squared error (MSE) as our cost function.\n",
    "\n",
    "# Code snippet for the cost function (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07abb973",
   "metadata": {},
   "source": [
    "\n",
    "# 4. Backpropagation Algorithm\n",
    "Now, we'll explore the backpropagation algorithm, which computes the gradients of the cost function with respect to the weights and biases of the network. We'll understand how errors are propagated backward through the network to update the parameters.\n",
    "\n",
    "# Code snippet for the backpropagation algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc642bca",
   "metadata": {},
   "source": [
    "\n",
    "# 5. Update Rule\n",
    "Finally, we'll implement the update rule to adjust the weights and biases of the network using the computed gradients and a learning rate.\n",
    "\n",
    "# Code snippet for the update rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b532513",
   "metadata": {},
   "source": [
    "# 6. Training Process\n",
    "We'll demonstrate the training process by applying backpropagation to a sample dataset. We'll iterate over the training data, compute gradients using backpropagation, and update the network parameters using stochastic gradient descent (SGD).\n",
    "\n",
    "# Code snippet for the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fedb48",
   "metadata": {},
   "source": [
    "# 7. Evaluation\n",
    "After training the network, we'll evaluate its performance on a test dataset to assess its accuracy and generalization.\n",
    "\n",
    "# Code snippet for evaluating the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0472d8e6",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Backpropagation is a fundamental algorithm for training neural networks, enabling them to learn from data and make accurate predictions. In this notebook, we explored the implementation of backpropagation in the provided neural network code, understanding its role in optimizing network parameters and improving performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
