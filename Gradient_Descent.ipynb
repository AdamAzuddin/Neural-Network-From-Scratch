{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be2c3a53",
   "metadata": {},
   "source": [
    "### Gradient Descent with Stochastic Gradient Descent (SGD)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll explore the implementation of stochastic gradient descent (SGD) as a method for training neural networks. SGD is a popular optimization algorithm used to minimize the cost function during the training process. We'll delve into the details of how SGD works and its role in updating the parameters of a neural network based on training data.\n",
    "\n",
    "## Content\n",
    "\n",
    "### 1. Stochastic Gradient Descent (SGD)\n",
    "We'll start by explaining the concept of stochastic gradient descent (SGD) and its differences from other gradient descent variants. We'll explore how SGD iteratively updates the parameters of the neural network using mini-batches of training data.\n",
    "\n",
    "### 2. Training Method\n",
    "We'll review the `SGD` method implemented in the provided neural network code. This method trains the neural network using mini-batch stochastic gradient descent. We'll discuss the parameters such as the number of epochs, mini-batch size, and learning rate (`eta`), and how they affect the training process.\n",
    "\n",
    "### 3. Initialization\n",
    "We'll initialize the neural network with random weights and biases, similar to the setup in the provided code. This step is crucial for starting the training process.\n",
    "\n",
    "### 4. Mini-Batch Creation\n",
    "We'll discuss how mini-batches are created from the training data and why mini-batch training is preferred over batch training or pure stochastic training.\n",
    "\n",
    "### 5. Updating Parameters\n",
    "We'll explore how the `update_mini_batch` method is used to update the weights and biases of the neural network based on the gradients computed from the mini-batch.\n",
    "\n",
    "### 6. Training Process\n",
    "We'll demonstrate the training process by applying SGD to a sample dataset. We'll iterate over the training data for multiple epochs, updating the parameters of the network using mini-batch SGD.\n",
    "\n",
    "### 7. Evaluation\n",
    "After training the network, we'll evaluate its performance on a separate test dataset to assess its accuracy and generalization. We'll compute metrics such as accuracy and loss to measure the network's performance.\n",
    "\n",
    "## Conclusion\n",
    "SGD is a powerful optimization algorithm for training neural networks, enabling them to learn from data and make accurate predictions. In this notebook, we explored the implementation of SGD in the provided neural network code, understanding its role in optimizing network parameters and improving performance.\n",
    "\n",
    "---\n",
    "\n",
    "Feel free to expand on each section with explanations, code snippets, and visualizations as needed."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
