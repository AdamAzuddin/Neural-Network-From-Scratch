{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70345f9",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we'll explore the concept of forward propagation in neural networks. Forward propagation is the process by which input data is passed forward through the network, layer by layer, to produce an output. We'll examine the implementation of forward propagation in the provided neural network code, which is encapsulated within a Python class.\n",
    "\n",
    "Readers should have a basic understanding of object-oriented programming (OOP) concepts to comprehend the structure of the network class.\n",
    "\n",
    "Throughout this notebook, we'll delve into the workings of forward propagation within this network class, understand how it computes the output of the network given an input, and provide detailed explanations and code snippets to facilitate understanding.\n",
    "\n",
    "Before we proceed, ensure that you have the NumPy library installed. If not, you can install it using the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e43e08a1",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\python311\\lib\\site-packages (1.26.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaef21b",
   "metadata": {},
   "source": [
    "First, here's the full code for the network class. You can run the cell below in one go and i'll explain it in more detail below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fba70c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data):\n",
    "\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "\n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test))\n",
    "            \n",
    "            else:\n",
    "                print(\"Epoch {} complete\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0dc8f7",
   "metadata": {},
   "source": [
    "# 1. Initialization\n",
    "First, let's start by initializing the neural network with random weights and biases. We'll create an instance of the Network class with a specified architecture (number of layers and neurons in each layer) and random initialization for weights and biases.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd6896",
   "metadata": {},
   "source": [
    "The first couple of lines are self-explanatory.The sizes parameter take a vector which contain the number of neurons in each layer. For example:\n",
    "\n",
    "```python\n",
    "net = network.Network([784, 50, 10])  #784 neurons in first layer, 50 neurons in the second layer and 10 neurons in the third layer\n",
    "```\n",
    "\n",
    "We first initialize the weights and biases with a random value within the Gaussian distribution, which means the values generated is of mean 0 and standard deviation of 1. \n",
    "Also, note that the weights and biases are in the form of a numpy array.\n",
    "\n",
    "The biases is a numpy array that contain the arrays of biases in each layer except the first layer because there is no biases in the first layer (we will call it as input layer from now, and the last layer is the output layer). For example:\n",
    "```python\n",
    "biases = [\n",
    "    # Biases for the second layer (index 0 corresponds to layer 1)\n",
    "    [0.1,  # Bias for neuron 1 in layer 2\n",
    "     0.2,  # Bias for neuron 2 in layer 2\n",
    "     0.3], # Bias for neuron 3 in layer 2\n",
    "\n",
    "    # Biases for the third layer (index 1 corresponds to layer 2)\n",
    "    [0.4,  # Bias for neuron 1 in layer 3\n",
    "     0.5]   # Bias for neuron 2 in layer 3\n",
    "]\n",
    "```\n",
    "\n",
    "The weights is kinda similar, but the the weights contain arrays of weights in each layer of neuron. For example:\n",
    "```python\n",
    "weights = [\n",
    "    # Weights for connections from the input layer to the hidden layer\n",
    "    [[0.1, 0.2],     # Weights for connections from neuron 1 in the input layer to neurons in the hidden layer\n",
    "     [0.3, 0.4],     # Weights for connections from neuron 2 in the input layer to neurons in the hidden layer\n",
    "     [0.5, 0.6]],    # Weights for connections from neuron 3 in the input layer to neurons in the hidden layer\n",
    "\n",
    "    # Weights for connections from the hidden layer to the output layer\n",
    "    [[0.5],          # Weights for connections from neurons in the hidden layer to the neuron in the output layer\n",
    "     [0.6]]          # Weights for connections from neurons in the hidden layer to the neuron in the output layer\n",
    "]\n",
    "```\n",
    "Here's the visual representation of the example network architecture:\n",
    "\n",
    "![Neural Network Architecture Example](./images/1.1.jpg)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec6dca9",
   "metadata": {},
   "source": [
    "# 2. Feedforward Function\n",
    "Next, we'll dive into the feedforward function, which performs the forward propagation through the network.The feedforward method iterates through each layer of the network, applying the weights and biases to the input and passing it through the activation function.The output of each layer becomes the input to the next layer, and this process continues until the final layer, producing the output of the network.\n",
    "\n",
    "This method(function of a class) takes an input vector and computes the output of the network by passing the input forward through each layer, applying the appropriate weights and biases, and applying the activation function (sigmoid) to produce the output of each neuron.Let's add the feed forward method to our network class.\n",
    "\n",
    "```python\n",
    "def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "```\n",
    "\n",
    "This one is pretty straightforward. When this method is called, first it will combine the weights and biases arrays using the zip function. Then, for every weights and biases(denoted as w and b), it will find the dot product between the weights and inputs and add the result with the biases array.Notice that all of this is wrapped around a custom function called sigmoid. Don't worry as this function will be clarified below. Finally, the method return the updated array a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699eda6b",
   "metadata": {},
   "source": [
    "### 3. Sigmoid Activation Function\n",
    "\n",
    "Before we proceed, let's briefly explain the sigmoid activation function used in the feedforward process. The sigmoid function maps any real-valued number to the range (0, 1), making it suitable for producing probabilities or binary outputs in classification tasks.\n",
    "\n",
    "```python\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "```\n",
    "\n",
    "The sigmoid graph looks like this:\n",
    "\n",
    "![Graph of sigmoid function](./images/1.2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd98d9a",
   "metadata": {},
   "source": [
    "### 4. Example\n",
    "Let's demonstrate the forward propagation process with an example. We'll provide an input vector to the network and observe how it produces the corresponding output using the feedforward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79204db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.67138945 0.68356306 0.7185193 ]]\n"
     ]
    }
   ],
   "source": [
    "net = Network([3,2,1])\n",
    "\n",
    "print(net.feedforward([0.4,1.2,1.9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa163e14",
   "metadata": {},
   "source": [
    "As you can see, the forward propagation give the output of an array whihc contain the array of two elements, corresponding to two neuron in the hidden layer. You can play around with the second value in the input vector when we first initialize the network and see how the output shape changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e369804",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Forward propagation is a fundamental process in neural networks where input data is passed forward through the network to produce an output. In this notebook, we explored how forward propagation is implemented in the provided neural network code, focusing on the feedforward method and the sigmoid activation function. Understanding forward propagation is essential for grasping the functioning of neural networks and their ability to make predictions based on input data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
